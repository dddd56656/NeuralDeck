import 'dart:async';
import 'dart:io';
import 'package:flutter/services.dart';
import 'package:path_provider/path_provider.dart';
import 'package:mediapipe_genai/mediapipe_genai.dart';

import 'brain_interface.dart';

/// éµå¾ªè°·æ­Œæ ‡å‡†çš„ MediaPipe æœ¬åœ°æ¨ç†å¤§è„‘
class LLMBrain implements BrainInterface {
  LlmInferenceEngine? _engine;
  bool _isInitialized = false;

  // æ¨¡å‹æ–‡ä»¶åï¼Œå¯¹åº”èµ„äº§ç›®å½•
  static const String _modelName = 'tinyllama.tflite'; // å®é™…ä¸ºä½ ä¸‹è½½çš„ .task æ–‡ä»¶

  @override
  Future<void> init() async {
    if (_isInitialized) return;
    print("ğŸ§  [Brain] æ­£åœ¨åˆå§‹åŒ–è°·æ­Œ MediaPipe æ¨ç†å¼•æ“...");

    try {
      final directory = await getApplicationDocumentsDirectory();

      // 1. èµ„æºå°±ä½ï¼šMediaPipe å¼•æ“éœ€è¦ç‰©ç†è·¯å¾„
      final modelFile = File('${directory.path}/$_modelName');
      if (!modelFile.existsSync()) {
        print("ğŸ“¦ [Brain] æ­£åœ¨æå–æ¨¡å‹èµ„æºåˆ°æœ¬åœ°å­˜å‚¨...");
        final data = await rootBundle.load('assets/models/$_modelName');
        await modelFile.writeAsBytes(
          data.buffer.asUint8List(data.offsetInBytes, data.lengthInBytes),
          flush: true,
        );
      }

      // 2. ç¼“å­˜ç›®å½•ï¼šç”¨äºå­˜æ”¾ KV Cache å’Œä¸­é—´å¼ é‡
      final cacheDir = Directory('${directory.path}/llm_cache');
      if (!cacheDir.existsSync()) await cacheDir.create();

      // 3. ç¡¬ä»¶é…ç½®ï¼šçº¢ç±³ Note 14 Pro å…·å¤‡ Mali-G615 GPU
      // æˆ‘ä»¬ä¼˜å…ˆä½¿ç”¨ GPU æ¨¡å¼ä»¥è·å¾—æ›´å¿«çš„ç”Ÿæˆé€Ÿåº¦
      final options = LlmInferenceOptions.gpu(
        modelPath: modelFile.path,
        maxTokens: 512, // é™åˆ¶æœ€å¤§ç”Ÿæˆé•¿åº¦
        temperature: 0.8, // æ§åˆ¶åˆ›é€ åŠ›
        topK: 40, // è¯é¢‘è¿‡æ»¤
        sequenceBatchSize: 1, // ç§»åŠ¨ç«¯å•ä¾‹æ‰¹å¤„ç†
      );

      // 4. å®ä¾‹åŒ–å¼•æ“
      _engine = LlmInferenceEngine(options);

      _isInitialized = true;
      print("âœ… [Brain] MediaPipe Engine å·²å°±ç»ª (GPU åŠ é€Ÿå·²æ¿€æ´»)");
    } catch (e) {
      print("âŒ [Brain] åˆå§‹åŒ–å¤±è´¥: $e");
      _isInitialized = false;
    }
  }

  @override
  Stream<String> generateLoreStream(String inputTags) async* {
    if (!_isInitialized || _engine == null) {
      await init();
    }

    // æ¸…ç†è¾“å…¥ï¼Œæ„é€ ç¬¦åˆæ¨¡å‹é¢„æœŸçš„ Prompt
    final String prompt = _buildPrompt(inputTags);
    print("ğŸ“ [Brain] å‘é€æŒ‡ä»¤è‡³æœ¬åœ°æ¨¡å‹: $prompt");

    try {
      // ğŸš€ ç›´æ¥è°ƒç”¨æºç ä¸­çš„ generateResponse æ¥å£
      // è¯¥æ¥å£è¿”å›çš„æ˜¯ Stream<String>ï¼Œå®Œç¾å¥‘åˆ Flutter çš„æµå¼ UI
      yield* _engine!.generateResponse(prompt).handleError((error) {
        print("âŒ [Brain] æ¨ç†æµå¼‚å¸¸: $error");
        return " [Link Error] ";
      });
    } catch (e) {
      print("âŒ [Brain] æ¨ç†å´©æºƒ: $e");
      yield " [Neural Link Failure] ";
    }
  }

  /// æ„é€ å¯¹è¯æ¨¡æ¿ï¼ˆé’ˆå¯¹ Gemma/TinyLlama ä¼˜åŒ–ï¼‰
  String _buildPrompt(String input) {
    return "<|user|>\nAnalyze this cyberpunk item: $input<|assistant|>\n";
  }

  @override
  Future<Map<String, dynamic>> analyzeTarget(String inputTags) async => {};

  @override
  void dispose() {
    // ğŸš€ éµå¾ªæºç è¦æ±‚ï¼šé‡Šæ”¾æ‰€æœ‰åŸç”Ÿèµ„æºï¼Œé˜²æ­¢ NDK å†…å­˜æ³„æ¼
    _engine?.dispose();
    _engine = null;
    _isInitialized = false;
    print("ğŸ§¹ [Brain] åŸç”Ÿèµ„æºå·²é‡Šæ”¾");
  }
}
