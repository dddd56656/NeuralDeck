import 'dart:async';
import 'dart:io';
import 'dart:math';
import 'package:flutter/services.dart';
import 'package:path_provider/path_provider.dart';
import 'package:fllama/fllama.dart';
import 'brain_interface.dart';

class LLMBrain implements BrainInterface {
  bool _isInitialized = false;
  double? _contextId;

  // âœ… 1. ç¡®ä¿æ–‡ä»¶åä¸€è‡´
  static const String _modelFileName = 'tinyllama.gguf';

  @override
  Future<void> init() async {
    if (_isInitialized && _contextId != null) return;
    print("ğŸ§  (TinyLlama): Init...");

    try {
      final directory = await getApplicationDocumentsDirectory();
      final modelPath = '${directory.path}/$_modelFileName';
      final file = File(modelPath);

      // æ¬è¿æ¨¡å‹ (Assets -> App Doc Dir)
      if (!file.existsSync()) {
        print("ğŸ“¦ æ­£åœ¨é‡Šæ”¾ TinyLlama æ¨¡å‹ (600MB+)...");
        try {
          final ByteData data = await rootBundle.load(
            'assets/models/$_modelFileName',
          );
          final List<int> bytes = data.buffer.asUint8List();
          await file.writeAsBytes(bytes, flush: true);
          print("âœ… æ¨¡å‹é‡Šæ”¾å®Œæˆ");
        } catch (e) {
          throw Exception(
            "âŒ æ‰¾ä¸åˆ° assets/models/tinyllama.ggufï¼Œè¯·æ£€æŸ¥ pubspec.yaml: $e",
          );
        }
      }

      // åˆå§‹åŒ–å¼•æ“
      print("ğŸš€ Loading Engine...");
      final result = await Fllama.instance()!.initContext(
        modelPath,
        nCtx: 2048, // TinyLlama æ”¯æŒ 2048
        nThreads: 4, // 4çº¿ç¨‹
        nGpuLayers: 0, // å¼ºåˆ¶ CPU
        emitLoadProgress: true,
      );

      if (result != null && result.containsKey('contextId')) {
        _contextId = (result['contextId'] as num).toDouble();
        print("âœ… Engine Ready! ID: $_contextId");
        _isInitialized = true;
      } else {
        throw Exception("Init failed: $result");
      }
    } catch (e) {
      print("âŒ Init Error: $e");
      rethrow;
    }
  }

  // è¿™é‡Œçš„ analyzeTarget ä¿æŒä¸å˜...
  @override
  Future<Map<String, dynamic>> analyzeTarget(String inputTags) async {
    if (!_isInitialized) await init();
    final seed = inputTags.codeUnits.fold(0, (p, c) => p + c);
    final rnd = Random(seed);
    double r() => (rnd.nextInt(90) + 10) / 100.0;
    await Future.delayed(const Duration(milliseconds: 100));
    return {"ATK": r(), "DEF": r(), "SPD": r(), "MAG": r(), "LUCK": r()};
  }

  @override
  Stream<String> generateLoreStream(String inputTags) {
    if (!_isInitialized || _contextId == null)
      return Stream.error("Brain not initialized");

    // âœ… 2. TinyLlama ä¸“ç”¨ Prompt æ ¼å¼ (éå¸¸é‡è¦ï¼)
    // å¿…é¡»ä¸¥æ ¼éµå®ˆ <|system|> ... </s> è¿™ç§æ ¼å¼
    final prompt =
        '''<|system|>
You are a Cyberpunk item analyzer. Describe the item in 1 sentence.</s>
<|user|>
Item: "$inputTags"</s>
<|assistant|>''';

    print("ğŸ“ Sending Prompt...");
    final controller = StreamController<String>();

    final sub = Fllama.instance()!.onTokenStream!.listen((event) {
      if (event['contextId'] == _contextId) {
        final token = event['token'] as String?;
        if (token != null) {
          // æ‰“å°åˆ°æ§åˆ¶å°çœ‹çœ‹æœ‰æ²¡æœ‰ååº”
          stdout.write(token);
          controller.add(token);
        }
        if (event['is_end'] == true || event['done'] == true) {
          print("\nâœ… Done");
          controller.close();
        }
      }
    }, onError: controller.addError);

    Fllama.instance()!
        .completion(
          _contextId!,
          prompt: prompt,
          nPredict: 50,
          emitRealtimeCompletion: true,
        )
        .catchError((e) {
          controller.addError(e);
          controller.close();
        });

    controller.onCancel = () => sub.cancel();
    return controller.stream;
  }

  @override
  Future<void> dispose() async {
    if (_contextId != null) {
      await Fllama.instance()!.releaseContext(_contextId!);
      _contextId = null;
    }
    _isInitialized = false;
  }
}
