import 'dart:async';
import 'dart:convert';
import 'dart:io';
import 'package:dio/dio.dart';
import 'package:path_provider/path_provider.dart';
import 'package:mediapipe_genai/mediapipe_genai.dart';
import 'brain_interface.dart';

class LLMBrain implements BrainInterface {
  bool _isInitialized = false;
  LlmInferenceEngine? _engine;

  // ğŸ“¡ æ¨¡å‹ä¸‹è½½åœ°å€
  // è¿™å¯ä»¥è®©å›½å†…è®¾å¤‡æ— éœ€æ¢¯å­ç›´æ¥é«˜é€Ÿä¸‹è½½
  static const String _modelUrl =
      "https://hf-mirror.com/google/gemma-2b-it-gpu-int4/resolve/main/gemma-2b-it-gpu-int4.bin";
  static const String _targetFileName = 'gemma-2b-it-gpu-int4.bin';

  @override
  Future<void> init() async {
    if (_isInitialized) return;
    print("ğŸ§  Neural Engine: Initializing Kernel...");

    try {
      final directory = await getApplicationDocumentsDirectory();
      final modelPath = '${directory.path}/$_targetFileName';
      // è·å–ç¼“å­˜ç›®å½•ï¼ˆä»… CPU æ¨¡å¼éœ€è¦ï¼‰
      final cachePath = directory.path;

      final file = File(modelPath);

      if (!file.existsSync()) {
        print("âš ï¸ ç¥ç»æ ¸å¿ƒä¸¢å¤±ï¼Œå¼€å§‹ä¸‹è½½...");
        await _downloadModel(modelPath);
        print("âœ… ä¸‹è½½å®Œæˆã€‚");
      } else {
        print("ğŸ“‚ å‘ç°æœ¬åœ°æ¨¡å‹: $modelPath");
      }

      // å¯åŠ¨å¼•æ“
      _igniteEngine(modelPath, cachePath);

      _isInitialized = true;
    } catch (e) {
      print("âŒ æ ¸å¿ƒå¯åŠ¨å¤±è´¥: $e");
      rethrow;
    }
  }

  Future<void> _downloadModel(String savePath) async {
    // âœ… ä¿®æ”¹ç‚¹ 2: å¢åŠ è¿æ¥è¶…æ—¶è®¾ç½®
    final dio = Dio(
      BaseOptions(
        connectTimeout: const Duration(seconds: 10), // è¿æ¥è¶…æ—¶ 10ç§’
        receiveTimeout: const Duration(minutes: 60), // ä¸‹è½½è¶…æ—¶ 60åˆ†é’Ÿ
      ),
    );
    try {
      await dio.download(
        _modelUrl,
        savePath,
        onReceiveProgress: (received, total) {
          if (total != -1) {
            final progress = (received / total * 100).toStringAsFixed(1);
            if (received % (total ~/ 20) < 100000) {
              print("â¬‡ï¸ ä¸‹è½½ä¸­: $progress%");
            }
          }
        },
        options: Options(receiveTimeout: const Duration(minutes: 30)),
      );
    } catch (e) {
      final file = File(savePath);
      if (file.existsSync()) file.deleteSync();
      throw Exception("ä¸‹è½½å¤±è´¥: $e");
    }
  }

  /// âœ… æ ¸å¿ƒä¿®å¤ï¼šæ ¹æ®æºç å®šä¹‰ï¼ŒåŒºåˆ†æ„é€ å‚æ•°
  void _igniteEngine(String modelPath, String cachePath) {
    LlmInferenceOptions options;
    try {
      print("ğŸš€ å°è¯•åŠ è½½ GPU æ¨¡å¼ (High Performance)...");

      // [GPU æ„é€ å™¨]
      // ä¾æ®æºç ï¼šéœ€è¦ sequenceBatchSizeï¼Œä¸éœ€è¦ cacheDir
      options = LlmInferenceOptions.gpu(
        modelPath: modelPath,
        sequenceBatchSize: 1, // å¿…å¡«
        maxTokens: 512,
        temperature: 0.7,
        topK: 40,
        randomSeed: 1024,
      );

      _engine = LlmInferenceEngine(options);
      print("âœ… GPU å¼•æ“ä¸Šçº¿ã€‚");
    } catch (e) {
      print("âš ï¸ GPU å¤±è´¥ ($e)ï¼Œåˆ‡æ¢è‡³ CPU (Standard)...");

      // [CPU æ„é€ å™¨]
      // ä¾æ®æºç ï¼šéœ€è¦ cacheDirï¼Œä¸éœ€è¦ sequenceBatchSize
      options = LlmInferenceOptions.cpu(
        modelPath: modelPath,
        cacheDir: cachePath, // å¿…å¡«
        maxTokens: 512,
        temperature: 0.7,
        topK: 40,
        randomSeed: 1024,
      );

      _engine = LlmInferenceEngine(options);
      print("âœ… CPU å¼•æ“ä¸Šçº¿ã€‚");
    }
  }

  @override
  Future<Map<String, dynamic>> analyzeTarget(String inputTags) async {
    _checkStatus();
    final prompt =
        '''<start_of_turn>user
Format: JSON {"ATK":0.0-1.0,"DEF":0.0-1.0,"SPD":0.0-1.0,"MAG":0.0-1.0,"LUCK":0.0-1.0}
Input: "$inputTags"
Output: JSON only.
<end_of_turn>
<start_of_turn>model
''';

    try {
      final responseStream = _engine!.generateResponse(prompt);
      final fullText = await responseStream.join();
      return json.decode(_extractJson(fullText));
    } catch (e) {
      return {"ATK": 0.5, "DEF": 0.5, "SPD": 0.5, "MAG": 0.5, "LUCK": 0.5};
    }
  }

  @override
  Stream<String> generateLoreStream(String inputTags) {
    _checkStatus();
    final prompt =
        '''<start_of_turn>user
Description for "$inputTags" (Cyberpunk style, max 20 words).
<end_of_turn>
<start_of_turn>model
''';
    return _engine!.generateResponse(prompt);
  }

  void _checkStatus() {
    if (!_isInitialized || _engine == null) {
      throw Exception("Neural Engine not initialized!");
    }
  }

  String _extractJson(String raw) {
    final start = raw.indexOf('{');
    final end = raw.lastIndexOf('}');
    if (start != -1 && end != -1) return raw.substring(start, end + 1);
    return "{}";
  }

  @override
  void dispose() {
    _engine?.dispose();
    _engine = null;
    _isInitialized = false;
  }
}
